from dotenv import load_dotenv
load_dotenv()

import os
import openai
from termcolor import colored  # d√©j√† dans requirements

# Chemin d'acc√®s adapt√© √† ta structure
from utils.llm_basic_context.kg_context_code_structure import KG_CONTEXT_CODE_STRUCTURE
from utils.llm_basic_context.kg_context_example_code import KG_CONTEXT_EXAMPLE_CODE
from utils.llm_basic_context.kg_context_rules import KG_CONTEXT_RULES

# ====== ENV par d√©faut ======
LLM_PROVIDER      = os.getenv("LLM_PROVIDER", "openai").lower()

OPENAI_MODELS_AVAILABLE = ["gpt-4o", "gpt-4o-mini"]  
OPENAI_API_KEY    = os.getenv("OPENAI_API_KEY", "")
DEFAULT_OAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o")

ASI1_MODELS_AVAILABLE = ["asi1-mini"]
ASI1_API_KEY      = os.getenv("ASI1_API_KEY", "")
ASI1_BASE_URL     = os.getenv("ASI1_BASE_URL", "https://api.asi1.ai/v1")
# ‚ö†Ô∏è lit d√©sormais ASI1_MODEL (conforme √† ton .env)
DEFAULT_ASI1_MODEL = os.getenv("ASI1_MODEL", "asi1-mini")

# Mod√®le par d√©faut selon provider
DEFAULT_MODEL = DEFAULT_ASI1_MODEL if LLM_PROVIDER == "asi1" else DEFAULT_OAI_MODEL
# =======================================================

class OTACON:
    """
    Agent central DECIMA pour :
      - G√©n√©ration de r√©ponses LLM (explication + code Python)
      - Injection du contexte EMMA (filtr√©) + contexte large structur√© (code structure MCNPTools)
      - Dialogue expert MCNP PTRAC (pas de hors sujet)
    """

    def __init__(self, model: str = DEFAULT_MODEL):
        # --- inchang√© : √©tat courant ---
        self.model = model
        self.provider = LLM_PROVIDER
        self.api_key = None
        self.client = None

        # Construction du client selon provider par d√©faut (.env)
        self._configure_client(self.provider)

    def _configure_client(self, provider: str) -> None:
        """
        (interne) Configure le client HTTP selon le provider (openai | asi1)
        sans toucher au reste de ta logique.
        """
        provider = (provider or "openai").lower()
        self.provider = provider
        if provider == "asi1":
            # Client OpenAI-compatible mais avec base_url ASI1
            self.api_key = ASI1_API_KEY
            self.client = openai.OpenAI(api_key=self.api_key, base_url=ASI1_BASE_URL)
        else:
            self.api_key = OPENAI_API_KEY
            self.client = openai.OpenAI(api_key=self.api_key)

    def set_model_and_provider(self, model_name: str) -> None:
        m = (model_name or "").lower()
        provider = "asi1" if m.startswith("asi") else "openai"

        # Validation si provider == asi1
        if provider == "asi1" and m not in ASI1_MODELS_AVAILABLE:
            print(colored(f"[WARN] Mod√®le ASI1 inconnu : {model_name}. "
                        f"Mod√®les valides : {', '.join(ASI1_MODELS_AVAILABLE)}", "red"))
            # Option : forcer sur le mod√®le par d√©faut
            model_name = DEFAULT_ASI1_MODEL

        if provider == "openai" and m not in OPENAI_MODELS_AVAILABLE:  # üÜï
            print(colored(f"[WARN] Mod√®le OpenAI inconnu : {model_name}. "
                        f"Mod√®les valides : {', '.join(OPENAI_MODELS_AVAILABLE)}", "red"))
            model_name = DEFAULT_OAI_MODEL

        self.model = model_name
        self._configure_client(provider)
        print(colored(f"[INFO] Reconfig LLM: provider={self.provider} | model={self.model}",
                    "green" if self.provider == "asi1" else "cyan"))

        # üîπ V√©rification imm√©diate aupr√®s de l‚ÄôAPI du mod√®le r√©ellement utilis√©
        if self.provider == "asi1":
            try:
                resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                )
                real_model = getattr(resp, "model", None)
                if real_model and real_model != self.model:
                    print(colored(f"[WARN] L'API a fallback sur un autre mod√®le : {real_model}", "red"))
                else:
                    print(colored(f"[DEBUG] Mod√®le confirm√© par l'API : {real_model}", "green"))
            except Exception as e:
                print(colored(f"[ERROR] Impossible de v√©rifier le mod√®le : {e}", "red"))

        
    def build_prompt(self, user_query: str, emma_context: dict, use_context: bool = True) -> str:
        prompt = []

        if use_context:
            prompt.extend([
                "Tu es OTACON, un agent MCNP PTRAC expert.",
                "Tu ne r√©ponds qu‚Äôaux requ√™tes d‚Äôanalyse ou parsing de fichiers PTRAC issus de MCNP, en utilisant exclusivement la librairie mcnptools.",
                "",
                "---",
                "CONTEXTE :",
                "Tu disposes de deux sources de contexte :",
                "1. Contexte EMMA : une liste d'entit√©s candidates extraites automatiquement du graphe de connaissances (KG).",
                "   Cette liste constitue un bon point de d√©part : les entit√©s les plus pertinentes y sont souvent pr√©sentes.",
                "   Toutefois, elle peut contenir des erreurs (faux positifs, faux n√©gatifs). Tu dois donc l'utiliser comme une aide √† la d√©cision, pas comme une v√©rit√© absolue.",
                "2. Contexte large structur√© : description exhaustive des classes, m√©thodes, enums et structures de mcnptools (fiable).",
                "   Si EMMA est incomplet ou erron√©, tu peux t‚Äôappuyer uniquement sur ce contexte structur√©.",
                "",
            ])

            # Ajout des r√®gles
            prompt.append(KG_CONTEXT_RULES.strip())
            prompt.append("")

            # Contexte EMMA
            if emma_context and "entities" in emma_context and emma_context["entities"]:
                prompt.append("[KG CONTEXT EMMA]")
                for ent in emma_context["entities"]:
                    s = f"- id: {ent['id']}, type: {ent.get('type', '')}"
                    if ent.get("parent_dict"):
                        s += f", parent_dict: {ent['parent_dict']}"
                    if ent.get("parent_enum"):
                        s += f", parent_enum: {ent['parent_enum']}"
                    if ent.get("parent_class"):
                        s += f", parent_class: {ent['parent_class']}"
                    if ent.get("value") is not None:
                        s += f", value: {ent['value']}"
                    if ent.get("score") is not None:
                        s += f", score: {ent['score']}"
                    if ent.get("description"):
                        s += f", description: {ent['description'][:150].replace(chr(10),' ')}"
                    prompt.append(s)
            else:
                prompt.append("(Pas d'entit√©s EMMA d√©tect√©es pour cette requ√™te.)")

            prompt.append("")
            prompt.append("# Contexte large structur√© (API et enums MCNPTools, √† utiliser si besoin ou pour validation crois√©e)")
            prompt.append(KG_CONTEXT_CODE_STRUCTURE.strip())
            prompt.append("")

            prompt.append("# Exemple typique d'analyse simple d'un fichier PTRAC MCNP avec mcnptools")
            prompt.append(KG_CONTEXT_EXAMPLE_CODE.strip())
            prompt.append("")

        else:
            prompt.append("Answer my query in relation to the attached PTRAC file, using Python and the mcnptools library.")
            prompt.append("")

        prompt.append("# Requ√™te utilisateur :")
        prompt.append(user_query)
        prompt.append("")

        return "\n".join(prompt)

    def ask_llm(self, prompt: str, temperature: float = 0.2, max_tokens: int = 2000) -> str:
    # Log de l'endpoint actif pour debug
        try:
            base_url = getattr(self.client, "_base_url", None) or getattr(self.client, "base_url", None)
            color = "green" if self.provider == "asi1" else "yellow"
            print(colored(f"[DEBUG] Endpoint API : {base_url}", color))
        except Exception:
            pass
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system",
                 "content": (
                    "Tu es un agent expert MCNP PTRAC. R√©ponds uniquement √† des questions de parsing/analyse de fichiers PTRAC MCNP. "
                    "Le contexte EMMA est un guidage, pas une v√©rit√© absolue. Utilise le contexte large structur√© si besoin. "
                    "Fournis toujours d‚Äôabord une explication, puis un code Python dans un bloc ```python."
                )},
                {"role": "user", "content": prompt},
            ],
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return response.choices[0].message.content

    def parse_llm_output(self, llm_output: str) -> dict:
        import re
        code = ""
        explanation = llm_output
        m = re.search(r"```python(.*?)```", llm_output, re.DOTALL)
        if m:
            code = m.group(1).strip()
            explanation = (
                llm_output[: m.start()].strip() + llm_output[m.end() :].strip()
            )
        return {
            "explanation": explanation.strip(),
            "code": code,
            "raw_output": llm_output,
        }

    def run(self, user_query: str, emma_context: dict) -> dict:
        # Print en temps r√©el le mod√®le qui va √™tre utilis√©
        provider_msg = f"[INFO] LLM utilis√© pour cette requ√™te : {self.provider} (mod√®le: {self.model})"
        print(colored(provider_msg, "cyan" if self.provider == "openai" else "green"))

        use_context = emma_context.get("use_context", True)
        prompt = self.build_prompt(user_query, emma_context, use_context=use_context)
        print("======= PROMPT LLM =======\n", prompt)
        llm_output = self.ask_llm(prompt)
        result = self.parse_llm_output(llm_output)
        return result

if __name__ == "__main__":
    user_query = (
        "Nombre total de neutrons, photons et photons de r√©actions photo-nucl√©aires traversant la cellule 200 repr√©sentant le f√ªt"
    )
    emma_context = {
        "entities": [
            # ‚Ä¶ liste structur√©e d‚Äôentit√©s (voir sortie EMMAAgent) ‚Ä¶
        ]
    }
    agent = OTACON()
    output = agent.run(user_query, emma_context)
    print("[EXPLICATION]")
    print(output["explanation"])
    print("[CODE PYTHON]")
    print(output["code"])
