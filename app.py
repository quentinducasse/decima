from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
from modules.campbell import CampbellOrchestrator
import os
from tools import sandbox

app = Flask(
    __name__,
    template_folder="frontend",        
    static_folder="frontend/static"   
)
CORS(app)

UPLOAD_FOLDER = os.path.join(os.getcwd(), "uploads")
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
app.config["UPLOAD_FOLDER"] = UPLOAD_FOLDER
app.config["PTRAC_PATH"] = None

campbell = CampbellOrchestrator()

@app.route("/")
def index():
    return render_template("index.html")

@app.route("/analyze_query", methods=["POST"])
def analyze():
    """
    Handle user query analysis. 
    Passes the query through QUIET → EMMA → OTACON → EVA depending on the workflow.
    """
    data = request.get_json()
    query = data.get("query", "")
    use_context = data.get("use_context", True)
    model_choice = data.get("model") or data.get("model_choice")
    ptrac_path = app.config.get("PTRAC_PATH", None)

    # Allow model override from frontend
    if model_choice:
        if hasattr(campbell.otacon_agent, "set_model_and_provider"):
            campbell.otacon_agent.set_model_and_provider(model_choice)
            if args.verbose:
                print(f"[APP] ✅ Model forced from frontend: {model_choice}")
                print(f"[APP] Active provider: {campbell.otacon_agent.provider} | "
                      f"Active model: {campbell.otacon_agent.model}")
        else:
            campbell.otacon_agent.model = model_choice
            if args.verbose:
                print(f"[APP] ⚠️ set_model_and_provider unavailable, model forced: {model_choice}")

    result = campbell.process_query(query, ptrac_path, use_context=use_context)
    return jsonify({
        "response": result.get("response", "No response."),
        "code": result.get("code", ""),
        "explanation": result.get("response", ""),   
        "execution_result": result.get("execution_result", {}),
        "error": result.get("error", "")            
    })

@app.route("/execute_code", methods=["POST"])
def execute():
    """
    Execute Python code generated by OTACON via EVA sandbox.
    """
    data = request.get_json()
    code = data.get("code", "")
    allow_plots = data.get("allow_plots", False) 
    ptrac_path = app.config.get("PTRAC_PATH", None)
    if not ptrac_path:
        return jsonify({"stderr": "No PTRAC file loaded", "stdout": "", "output_files": []})
    from modules.eva import EVA
    eva = EVA()
    eva.load_file(ptrac_path)
    result = eva.execute_code(code, allow_plots=allow_plots) 
    return jsonify(result)

@app.route("/data/ptrac_samples", methods=["POST"])
def upload_ptrac():
    """
    Upload PTRAC sample file to server (used for analysis).
    """
    if "file" not in request.files:
        return jsonify({"status": "error", "message": "No file provided."})
    file = request.files["file"]
    if file.filename == "":
        return jsonify({"status": "error", "message": "Empty filename."})
    if file:
        path = os.path.join(app.config["UPLOAD_FOLDER"], file.filename)
        file.save(path)
        app.config["PTRAC_PATH"] = path
        return jsonify({"status": "success", "path": path, "filename": file.filename})
    return jsonify({"status": "error", "message": "File save error."})

@app.route("/abort_execution", methods=["POST"])
def abort_execution():
    """
    Abort currently running code execution (if any).
    """
    if sandbox.ACTIVE_PROCESS:
        sandbox.ACTIVE_PROCESS.terminate()
        sandbox.ACTIVE_PROCESS = None
        return jsonify({"status": "terminated"})
    return jsonify({"status": "no_active_process"})

if __name__ == "__main__":
    import argparse
    import logging

    parser = argparse.ArgumentParser()
    parser.add_argument("-v", "--verbose", action="store_true", help="Enable verbose output")
    args = parser.parse_args()

    if args.verbose:
        # Verbose mode: DEBUG logs visible
        logging.basicConfig(
            level=logging.DEBUG,
            format="%(asctime)s - %(levelname)s - %(message)s",
            force=True  # <— important: override Flask's logging
        )
        app.logger.setLevel(logging.DEBUG)

        # Suppress DEBUG noise from external libs
        logging.getLogger("neo4j").setLevel(logging.WARNING)
        logging.getLogger("httpx").setLevel(logging.WARNING)
        logging.getLogger("httpcore").setLevel(logging.WARNING)
        logging.getLogger("openai").setLevel(logging.WARNING)

        app.run(debug=True, host="0.0.0.0", port=5050)

    else:
        # Silent mode: INFO logs only
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            force=True
        )
        log = logging.getLogger("werkzeug")
        log.setLevel(logging.INFO) 
        app.logger.setLevel(logging.INFO)

        # Suppress external libs as well
        logging.getLogger("neo4j").setLevel(logging.WARNING)
        logging.getLogger("httpx").setLevel(logging.WARNING)
        logging.getLogger("httpcore").setLevel(logging.WARNING)
        logging.getLogger("openai").setLevel(logging.WARNING)

        # version w/o docker
        # app.run(debug=False, port=5050)
        # version w/ docker
        app.run(debug=False, host="0.0.0.0", port=5050)

