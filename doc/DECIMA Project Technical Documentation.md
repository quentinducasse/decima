# DECIMA Project Technical Documentation

**Author:** DECIMA Team

**Date:** 1er Sept 2025

## 1. Introduction

The DECIMA (Data Extraction & Contextual Inference for MCNP Analysis) project is an advanced solution designed to facilitate the analysis and interpretation of particle track (PTRAC) files generated by the MCNP Monte Carlo transport code. It aims to transform natural language user queries into complex data analyses by generating and executing Python code via the `mcnptools` library. DECIMA positions itself as an intelligent assistant for engineers and researchers handling nuclear simulation data, automating extraction and processing tasks that would otherwise be tedious and prone to manual errors.

The primary objective of DECIMA is to democratize access to information contained in PTRAC files, allowing users to ask intuitive questions without requiring in-depth expertise in programming or the internal structure of PTRAC files. The system orchestrates several specialized modules, each contributing to a key step in the process, from query understanding to code execution and result presentation.

This document provides a detailed technical overview of DECIMA's architecture, its software components, their interactions, and underlying technologies. It is intended for developers, software architects, and anyone wishing to understand the system's internal workings for maintenance, extension, or debugging purposes.




## 2. Global Architecture

DECIMA's architecture is modular and relies on an agent-based approach, orchestrated by a central component. This design allows for great flexibility, increased maintainability, and the possibility of integrating new functionalities or replacing existing modules without affecting the entire system. The core orchestration is managed by LangGraph, a library that facilitates the construction of state graphs for complex workflows based on language models.

The system is structured around several main modules, each with a well-defined responsibility:

*   **QUIET (Query Preliminary Analysis):** Responsible for language detection, keyword extraction, and relevant entity identification from the user query. It prepares the query for subsequent steps by identifying focuses (events, data, classes, methods, attributes) and user intentions.
*   **EMMA (Knowledge Graph Contextual Extraction):** Interacts with a Neo4j database (Knowledge Graph) to enrich the query with contextual information. It uses the entities identified by QUIET to retrieve details about `mcnptools` classes, enumerations, dictionaries, and relationships, thus providing a rich semantic context for the LLM.
*   **OTACON (Response and Code Generation):** The central module based on a Large Language Model (LLM). It receives the user query enriched by EMMA's context and generates a natural language explanation as well as the corresponding Python code. OTACON is trained to produce accurate and relevant `mcnptools` code for analyzing PTRAC files.
*   **EVA (Execution and Validation):** Executes the Python code generated by OTACON in a sandboxed environment. It manages PTRAC file loading, secure code execution, output capture (stdout, stderr), and error handling. EVA is crucial for ensuring the security and reliability of the LLM-generated code execution.
*   **CAMPBELL (Main Orchestrator):** The system's orchestrator. It coordinates the flow of information between QUIET, EMMA, OTACON, and EVA, managing state transitions and ensuring that each module receives appropriate inputs and produces expected outputs. CAMPBELL is implemented with LangGraph for robust workflow management.

In addition to these main modules, DECIMA integrates a web user interface (frontend) built with Flask, HTML, CSS, and JavaScript, allowing users to interact with the system intuitively. An `uploads` folder is dedicated to temporary management of PTRAC files uploaded by the user. Utilities (`utils`) and test scripts (`tests`) complete the project's ecosystem.

The interaction between these components follows a sequential pipeline: the user query is first analyzed, then enriched by context, code is generated, and finally executed. The execution results are then presented to the user via the web interface. This architecture ensures a clear separation of concerns and high scalability for future project developments.




## 3. Detailed Components

This section describes in detail the role, functionalities, and interactions of each key module in DECIMA.

### 3.1. QUIET (Query Preliminary Analysis)

**File:** `modules/quiet.py`

The QUIET module is the first step in the user query processing pipeline. Its role is to analyze the natural language query to extract structured information that will guide subsequent modules. It performs language detection, tokenization, normalization, and key entity extraction.

**Main functionalities:**

*   **Language detection (`detect_language`):** Identifies whether the query is in French or English based on a set of language-specific keywords. This allows for adaptation of subsequent processing, particularly the use of stopwords and the bilingual lexicon.
*   **Query preprocessing (`preprocess_query`):** Converts the query to lowercase, removes punctuation and language-specific stopwords. It also handles the extraction of complex keywords such as MCNP identifiers (e.g., `MT-101`).
*   **Focus extraction (`extract_focus_events`, `extract_focus_data`, etc.):** This is QUIET's most critical function. It uses keyword dictionaries (`FOCUS_EVENTS`, `FOCUS_DATA`, `FOCUS_DICTIONARIES`, `FOCUS_CLASSES`, `FOCUS_METHODS`, `FOCUS_ATTRIBUTES`) to identify relevant business concepts mentioned in the query. These dictionaries are defined in `utils/keywords_quiet.py` and `utils/keywords_fr_en.py`. The extraction considers plural and singular forms, as well as accents, for increased robustness.
    *   `FOCUS_EVENTS`: Identifies PTRAC event types (Source, Bank, Collision, Termination, Surface, Lost).
    *   `FOCUS_DATA`: Detects PTRAC data fields (NPS, Energy, Time, X, Y, Z, Particle, Cell, Surface, Tally, etc.).
    *   `FOCUS_DICTIONARIES`: Recognizes references to Knowledge Graph dictionaries (ParticleCodeDict, PtracZAIDDict, PtracReactionDict).
    *   `FOCUS_CLASSES`, `FOCUS_METHODS`, `FOCUS_ATTRIBUTES`: Identifies `mcnptools` library classes, methods, and attributes mentioned in the query.

**Output:** The QUIET module returns a dictionary containing the original query, the detected language, and lists of identified entities for each focus category. This dictionary is then passed to EMMA for contextual enrichment.

**Example of operation:**
If the query is: *"What is the x, y, z position and energy of neutrons crossing surface 300?"*
QUIET will detect:
*   `language`: `en`
*   `focus_data`: `["X", "Y", "Z", "ENERGY", "PARTICLE", "SURFACE"]`
*   `focus_dictionaries`: `["ParticleCodeDict"]` (for 'neutrons')
*   `focus_events`: `["SUR"]` (for 'crossing surface')

This structured information is essential for EMMA to query the Knowledge Graph relevantly and for OTACON to generate appropriate Python code.

### 3.2. EMMA (Knowledge Graph Contextual Extraction)

**File:** `modules/emma.py`

The EMMA module is responsible for extracting relevant context from the Neo4j-based Knowledge Graph (KG). It uses the structured information provided by QUIET to query the KG and retrieve details about entities (classes, enumerations, methods, attributes, dictionary values) that match the user's query. The goal is to provide OTACON with a rich and precise semantic context, thereby reducing the risk of hallucinations or errors in the generated code.

**Main functionalities:**

*   **Neo4j Connection:** EMMA connects to a Neo4j instance using authentication information defined in environment variables. `neo4j_loader.py` is used to manage this connection and basic graph operations.
*   **Entity Extraction (`extract_kg_context`):** This is EMMA's central function. It takes the focus dictionary generated by QUIET as input and performs Cypher queries on the KG. For each focus type (events, data, dictionaries, classes, methods, attributes), EMMA searches for corresponding nodes in the graph.
*   **Specific Dictionary Management (MT, Particle, ZAID):** EMMA implements advanced scoring logic for `PtracReactionDict` (MT_xxx), `ParticleCodeDict`, and `PtracZAIDDict` dictionaries. This allows for ranking the most relevant entities based on the query, taking into account:
    *   Explicit MT numbers (e.g., `MT_101`).
    *   Mentioned particles (e.g., `neutron`, `photon`).
    *   Reaction patterns (e.g., `(n,p)`, `n,3a`).
    *   General keywords (e.g., `fission`, `capture`).
    This discriminatory approach ensures that only the most relevant entities are passed to OTACON, thus avoiding excessive "noise" in the context.
*   **Entity Enrichment:** For each entity found in the KG, EMMA retrieves complete information such as its type, description, parent class, parent enumeration, and parent dictionary. This information is crucial for OTACON to understand the semantics of the entities and generate correct code.
*   **Normalization and Lexicon:** EMMA uses the bilingual lexicon (`FR_TO_EN_LEXICON` in `utils/keywords_fr_en.py`) to normalize keywords and ensure that KG queries are consistent, regardless of the original language of the user query.

**Output:** EMMA returns an `emma_context` dictionary containing a list of enriched entities. Each entity is a dictionary with fields such as `id`, `type`, `description`, `parent_class`, `parent_enum`, `parent_dict`, and a `score` indicating its relevance. This context is directly injected into OTACON's prompt.

### 3.3. OTACON (Response and Code Generation)

**File:** `modules/otacon.py`

OTACON is the brain of DECIMA, an agent based on a Large Language Model (LLM) that transforms the user query and EMMA context into a natural language explanation and executable Python code. It is designed to be an expert in MCNP PTRAC file analysis and the use of the `mcnptools` library.

**Main functionalities:**

*   **Prompt Construction (`build_prompt`):** OTACON constructs a detailed prompt for the LLM by combining several sources of information:
    *   **System Instructions:** Defines OTACON's role (MCNP PTRAC expert), its limitations (only respond to PTRAC analysis/parsing queries, exclusively use `mcnptools`), and the expected output format (explanation + Python code block).
    *   **EMMA Context:** The list of relevant entities extracted by EMMA is directly inserted into the prompt. OTACON is instructed to use this context as a guide, but not to consider it as absolute truth, allowing it to rely on its own knowledge or the structured broad context if EMMA is incomplete or erroneous.
    *   **Structured Broad Context (`KG_CONTEXT_CODE_STRUCTURE`):** An exhaustive and reliable description of `mcnptools` classes, methods, enumerations, and structures is provided. This includes method signatures, return types, and relationships between classes. This context is crucial for ensuring the generation of syntactically and semantically correct code.
    *   **Code Example (`KG_CONTEXT_EXAMPLE_CODE`):** A complete example of PTRAC analysis with `mcnptools` is included to guide the LLM on good coding practices and the general structure of the Python script.
    *   **Rules and Best Practices (`KG_CONTEXT_RULES`):** Strict rules are imposed on the LLM, such as prohibiting the import of `mcnptools` classes other than `Ptrac`, the obligation to use public methods rather than internal attributes, and the use of a placeholder for the PTRAC file path (`<PTRAC_PATH_PLACEHOLDER>`). Best practices for particle tracking and calculation of physical quantities (deposited energy, time of flight) are also provided to avoid common errors.
    *   **User Query:** The original user query is added at the end of the prompt.
*   **LLM Call (`ask_llm`):** OTACON uses the OpenAI API (or a compatible model) to query the LLM with the constructed prompt. The temperature is set to 0.2 to favor more deterministic and less creative responses, which is desirable for technical code generation.
*   **LLM Output Parsing (`parse_llm_output`):** The raw LLM response is analyzed to separate the natural language explanation from the Python code block. The code is extracted if it is enclosed by ```python...``` tags.

**Output:** OTACON returns a dictionary containing the LLM-generated explanation, the extracted Python code, and the raw LLM output for debugging. The Python code is then passed to EVA for execution.

### 3.4. EVA (Code Execution and Validation)

**File:** `modules/eva.py` and `tools/sandbox.py`

EVA is the module responsible for executing the Python code generated by OTACON in a secure environment and capturing the results. It relies on the `sandbox.py` module for isolation and execution management.

**Main functionalities:**

*   **PTRAC File Loading (`load_file`):** Takes the path to the PTRAC file uploaded by the user as a parameter. It checks for the file's existence and makes it available for code execution.
*   **Secure Code Execution (`execute_code`):** This is EVA's key function, which delegates most of the work to `run_ptrac_code` in `tools/sandbox.py`.
    *   **PTRAC Path Substitution:** The `<PTRAC_PATH_PLACEHOLDER>` in the code generated by OTACON is replaced with the actual path of the PTRAC file loaded by the user.
    *   **PTRAC Mode Detection:** Before execution, `sandbox.py` automatically detects whether the PTRAC file is binary (`BIN_PTRAC`) or ASCII (`ASC_PTRAC`) and adapts the `Ptrac` object instantiation accordingly. This ensures compatibility with different MCNP file formats.
    *   **Code Patching:** `sandbox.py` modifies the generated code to:
        *   Ensure that all `Ptrac` instantiations use the detected mode (`patch_ptrac_instantiation`).
        *   Disable plotting functions (`plt.show()`, `plt.savefig()`, etc.) if `allow_plots` is `False`, to avoid unwanted graphical interactions in the sandboxed environment (`patch_disable_plots`).
    *   **Sandbox Execution:** The code is written to a temporary file and executed in an isolated Python subprocess. This prevents the generated code from accessing the host file system or performing dangerous operations. A timeout is applied to prevent infinite loops or excessively long executions.
    *   **Output Capture:** `stdout` (standard output) and `stderr` (standard error) from the sandboxed process are captured. Any exception raised during execution is also recorded.
    *   **Interruption Handling:** The `sandbox.py` module manages an active process (`ACTIVE_PROCESS`) that can be manually terminated via the Flask application's `/abort_execution` API, allowing the user to stop a blocked code execution.

**Output:** EVA returns a dictionary containing `success` (boolean indicating whether execution was successful), `stdout`, `stderr`, `exception` (if any), and `output_files` (for generated files, although not fully utilized yet for plots).

### 3.5. CAMPBELL (Main Orchestrator)

**File:** `modules/campbell.py`

CAMPBELL is the central module of DECIMA, acting as the main orchestrator of the workflow. It is built using the LangGraph library, which allows it to define a state graph to manage the sequence of calls between QUIET, EMMA, OTACON, and EVA. This graph-based approach offers great flexibility for complex and conditional workflows.

**Main functionalities:**

*   **State Definition (`AgentState`):** CAMPBELL defines a `TypedDict` structure (`AgentState`) that represents the global state of the workflow. This state contains all relevant information passed from one module to another, such as the user query, language, extracted focuses, EMMA context, LLM response, generated code, PTRAC file path, and execution results.
*   **Workflow Creation (`_create_workflow`):** Uses LangGraph's `StateGraph` to define the nodes (each module is a node) and transitions between them. The workflow is sequential but includes a conditional transition after OTACON:
    *   `quiet` -> `emma`
    *   `emma` -> `otacon`
    *   `otacon` -> `eva` (if code has been generated and a PTRAC file is provided)
    *   `otacon` -> `END` (if no code has been generated or no PTRAC file is provided)
    *   `eva` -> `END`
*   **Workflow Nodes (`_run_quiet_agent`, `_run_emma_agent`, etc.):** Each method encapsulates the call to a specific module (QUIET, EMMA, OTACON, EVA) and updates the global workflow state with the module's execution results. Error handling is integrated into each node, allowing exceptions to be caught and the workflow status to be updated accordingly.
*   **Conditional Transition (`_route_after_otacon`):** This routing function determines whether the workflow should proceed to EVA (to execute the code) or terminate directly after OTACON. This decision is based on the presence of code generated by OTACON and the availability of a PTRAC file.
*   **Main API (`process_query`):** The `process_query` method is CAMPBELL's public entry point. It initializes the workflow state with the user query and PTRAC file path, then invokes the LangGraph state graph. It returns the final workflow state, containing all information generated by the different modules.

**Interactions with the Flask application (`app.py`):** CAMPBELL is instantiated once at Flask application startup. The `/analyze_query` and `/execute_code` routes of the Flask application call `campbell.process_query` and `eva.execute_code` respectively (via a direct instantiation of EVA for manual code execution). This allows the web interface to trigger the DECIMA workflow and retrieve results.




### 3.6. Frontend (User Interface)

**Files:** `frontend/index.html`, `frontend/static/js/main.js`, `frontend/static/css/style.css`

DECIMA's user interface is a simple and intuitive web application, built with Flask for the backend and standard web technologies (HTML, CSS, JavaScript) for the frontend. It allows users to upload PTRAC files, submit natural language queries, and visualize LLM responses as well as code execution results.

**Key components:**

*   **`index.html`:** The main HTML file that defines the page structure. It includes links to CSS stylesheets and JavaScript scripts, and contains user interface elements such as the file upload button, query input field, chat message display area, and blocks for code execution results.
*   **`style.css`:** The CSS stylesheet that manages the visual appearance of the application. It defines colors, fonts, layouts (flexbox for section organization), and styles for various interface elements to provide a pleasant user experience consistent with DECIMA's visual identity.
*   **`main.js`:** The JavaScript script that handles frontend interactivity. Its main functionalities include:
    *   **PTRAC file upload:** Manages the selection and sending of the PTRAC file to the backend via a `POST` request to the `/data/ptrac_samples` endpoint. It updates the interface to display the name of the loaded file.
    *   **Query submission:** Captures the query entered by the user and sends it to the backend via a `POST` request to the `/analyze_query` endpoint. It displays the user's query in the chat area and activates a loading indicator (`reasoning-loader`).
    *   **LLM response display:** Receives the response from the backend (explanation and Python code) and displays it in the chat area. The Python code is formatted with `highlight.js` for better readability.
    *   **Generated code execution:** An "Execute code" button is dynamically added below the Python code block. When the user clicks it, the code is sent to the backend via a `POST` request to the `/execute_code` endpoint. The script also handles the display of `stdout`, `stderr` outputs, and any output files (like plot images) in the results section.
    *   **Interruption handling:** A "Stop Code Generation" button is implemented to allow the user to interrupt the execution of long or stuck code via the `/abort_execution` endpoint.
    *   **Context management:** A "Add context" checkbox allows the user to choose whether EMMA context should be used or not when OTACON analyzes the query.

The interface is designed to be responsive and provide a smooth user experience, with visual indicators for ongoing operations and clear formatting of information.

### 3.7. Knowledge Graph (Neo4j)

**Files:** `kg/loader/neo4j_loader.py`, `kg/triplets/*.json`

The Knowledge Graph (KG) is a fundamental component of DECIMA, providing a structured representation of knowledge related to the `mcnptools` library and MCNP/PTRAC concepts. It is implemented using Neo4j, a graph database, which allows for efficient modeling of complex relationships between entities.

**KG Structure:**

The KG is populated from JSON files (`kg/triplets/*.json`) that describe triplets (Subject, Predicate, Object). These triplets represent entities (nodes) and the relationships (edges) between them. Entity types include:

*   **Classes:** Represent `mcnptools` Python classes (e.g., `Ptrac`, `PtracEvent`, `PtracHistory`).
*   **Enums:** Represent MCNP enumerations (e.g., `PtracEventType`, `PtracBankType`, `PtracDataType`).
*   **EnumValues:** Represent specific values within enumerations (e.g., `SRC`, `BNK`, `TER` for `PtracEventType`).
*   **Methods:** Represent methods of `mcnptools` classes (e.g., `GetNPS()`, `Has(DATA)`).
*   **Attributes:** Represent class attributes.
*   **Dictionaries:** Represent mapping dictionaries (e.g., `ParticleCodeDict`, `PtracReactionDict`, `PtracZAIDDict`).
*   **ParticleCode, ReactionCode, ZAIDCode:** Represent specific codes (particles, reactions, ZAID) and their descriptions.

Relationships between these entities include `HAS_ENUM`, `BELONGS_TO_ENUM`, `IS_METHOD_OF`, `HAS_ATTRIBUTE`, `BELONGS_TO_DICT`, etc., allowing for navigation and querying of the graph to extract contextual information.

**`neo4j_loader.py`:** This script is responsible for migrating data from the JSON triplet files to the Neo4j database. It performs the following operations:

*   **Connection:** Establishes a connection with the Neo4j instance.
*   **Deletion of existing data:** At the beginning of each migration, all existing data in the graph is deleted to ensure a clean base.
*   **Constraint creation:** Defines uniqueness constraints on node names to ensure data integrity and optimize queries.
*   **Node creation:** Iterates through all triplets to identify subjects and objects, and creates corresponding nodes in the graph with appropriate labels (Class, Enum, EnumValue, etc.).
*   **Property addition:** Adds properties (description, value, etc.) to the nodes.
*   **Relationship creation:** Establishes relationships between nodes based on triplet predicates. Specific logic is applied for certain predicates (`has_enum`, `belongs_to_enum`) to ensure label consistency.

The KG is an essential resource for EMMA, allowing it to provide OTACON with precise and relevant semantic context, which is crucial for generating high-quality code and responses.




## 4. The `mcnptools` Library

The `mcnptools` library is at the heart of the DECIMA project, serving as the foundation for PTRAC file analysis. It is a Python library (with underlying C++ components for performance) specifically designed for reading, parsing, and manipulating output data from MCNP simulations, particularly PTRAC (Particle Track) files. It offers a programmatic interface to access detailed information on particle trajectories, collision events, banks, terminations, and other relevant data.

**Key `mcnptools` Concepts:**

*   **`Ptrac`:** The main class for interacting with a PTRAC file. It allows opening the file, reading its header, and retrieving particle histories (`PtracHistory`). It also handles different PTRAC file formats (binary, ASCII).
*   **`PtracHistory`:** Represents a single particle history (or trajectory) in the PTRAC file. A history is a sequence of events that describe the path of a particle and its generated secondaries during a simulation. This class provides methods to access the number of events in the history and to retrieve individual events (`PtracEvent`).
*   **`PtracEvent`:** Represents a unique event in a particle history. An event can be a source (SRC), a bank (BNK), a collision (COL), a surface crossing (SUR), a termination (TER), or a loss (LST). Each event contains specific data (position, energy, time, particle type, etc.) that can be accessed via generic methods (`Has`, `Get`).
*   **`PtracNps`:** Contains summary information about a particle history, such as the number of simulated particles (NPS), the cell, surface, associated tally, and a general value. This class is often used for aggregated analyses or to filter histories.
*   **Enumerations (`PtracEnums`):** `mcnptools` uses enumerations to represent event types (`PtracEventType`), bank types (`PtracBankType`), termination types (`PtracTerminationType`), and data types (`PtracDataType`). These enumerations are crucial for correctly interpreting raw data from PTRAC files and for writing readable and robust code.

**Interaction with DECIMA:**

In DECIMA, `mcnptools` is primarily used by the EVA module to execute Python code generated by OTACON. OTACON, in turn, is informed of the structure and capabilities of `mcnptools` via the structured broad context and EMMA context, which allows it to generate valid and efficient Python code that interacts correctly with the library. `sandbox.py` ensures that `Ptrac` instantiations are correct by detecting the PTRAC file mode (binary or ASCII) and patching the code if necessary. This tight integration allows DECIMA to translate complex queries into precise operations on PTRAC data.

## 5. Deployment and Execution

The DECIMA project is a Flask application that can be deployed in a standard Python environment. Here are the general steps for deployment and execution:

### 5.1. Prerequisites

*   **Python 3.8+:** The project is developed in Python and requires a compatible version.
*   **Neo4j:** A Neo4j database instance is required for the Knowledge Graph. It can be installed locally or accessed via a cloud service.
*   **OpenAI API Key:** A valid API key to access OpenAI language models (or a compatible service) is required for the OTACON module.

### 5.2. Installing Dependencies

All Python dependencies are listed in the `requirements.txt` file. They can be installed using `pip`:

```bash
pip install -r requirements.txt
```

It is recommended to use a virtual environment (`venv` or `conda`) to isolate project dependencies.

### 5.3. Environment Configuration

The following environment variables must be configured:

*   `OPENAI_API_KEY`: Your OpenAI API key.
*   `NEO4J_URI`: The URI of your Neo4j instance (e.g., `bolt://localhost:7687`).
*   `NEO4J_USER`: The username for connecting to Neo4j (e.g., `neo4j`).
*   `NEO4J_PASSWORD`: The password for connecting to Neo4j (e.g., `decima123`).

These variables can be set directly in the system environment or in a `.env` file at the project root, which will be loaded by the `python-dotenv` library.

### 5.4. Loading the Knowledge Graph

Before launching the application, the Knowledge Graph must be loaded into Neo4j. This is done by running the `neo4j_loader.py` script:

```bash
python kg/loader/neo4j_loader.py
```

This script will clean the existing Neo4j database and populate it with the triplets defined in the `kg/triplets` folder.

### 5.5. Launching the Flask Application

The Flask application can be launched by running the `app.py` file as follows:

```bash
python app.py
```

By default, the application will be accessible at `http://127.0.0.1:5050`. `debug=True` mode is enabled for development, which allows automatic server reloading upon code changes and provides detailed debugging information.

### 5.6. Directory Structure

The project follows a well-defined directory structure to organize the different components:

```
.
├── app.py                     # Main Flask application entry point
├── data/                      # C++ files and MCNP examples (ptrac_samples, mctal_samples)
├── doc/                       # Documentation (this file)
├── frontend/                  # Web user interface (HTML, CSS, JS)
│   ├── index.html
│   ├── static/
│   │   ├── css/
│   │   └── js/
│   └── pics/                  # Images for the frontend
├── kg/                        # Knowledge Graph backend (Neo4j)
│   ├── loader/
│   │   └── neo4j_loader.py    # KG loading script
│   └── triplets/              # KG triplets JSON files
├── modules/                   # Functional modules (agents)
│   ├── campbell.py            # Main orchestrator
│   ├── emma.py                # Contextual extraction agent (KG)
│   ├── eva.py                 # Code execution agent
│   ├── otacon.py              # Response/code generation agent (LLM)
│   └── quiet.py               # Query analysis agent
├── tests/                     # Unit and integration tests
├── tools/                     # Internal tools (sandbox)
│   └── sandbox.py             # Secure code execution
├── uploads/                   # Folder for uploaded PTRAC files
└── utils/                     # Support files (NLP, LLM context)
    ├── keywords_fr_en.py      # Bilingual lexicon and stopwords
    ├── keywords_quiet.py      # Keywords for QUIET
    └── llm_basic_context/     # Static context for OTACON
        ├── kg_context_code_structure.py
        ├── kg_context_example_code.py
        └── kg_context_rules.py
```

This structure facilitates project navigation and understanding of the organization of different modules.

## 6. Conclusion

The DECIMA project represents a significant step forward in automating nuclear simulation data analysis. By combining natural language processing techniques, graph databases, and advanced language models, it offers a powerful and intuitive solution for querying and analyzing PTRAC files. Its modular architecture and agent-based approach ensure great flexibility and future scalability. DECIMA paves the way for more natural and efficient interaction with complex scientific data, reducing the technical barrier for users and accelerating the research and engineering process.



